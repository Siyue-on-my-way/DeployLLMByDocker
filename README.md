# 分支指引

## ollama
### ollama/deploy_origin_qwen2_0.5b
> ollama部署qwen模型的方式，主要是直接部署ollama库里的模型，直接执行ollama run  {model_name}会自动下载模型并启动模型服务

### ollama/deploy_download_model
> ollama部署本地大模型的方式，主要是在hf-mirror或huggingface网站上手动下载大模型文件，然后尝试在本地部署时使用这些模型文件

## xinference


## vllm


