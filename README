- docker-copomse 启动ollama并暴露服务端口
- 进入docker ollama 
```shell
docker exec -it ollama bash
```
- 使用ollama命令启动大模型
```
ollama run qwen2:0.5b
```

- 测试
1、生成回复
```
curl http://118.178.241.227:11434/api/generate -d '{
  "model": "qwen2:0.5b",
  "prompt":"Why is the sky blue?",
  "stream":false
}'
```

2、模型对话
```
curl http://118.178.241.227:11434/api/chat -d '{
  "model": "qwen2:0.5b",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ],
  "stream":false
}'
```